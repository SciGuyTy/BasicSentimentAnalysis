{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "Author: Tyler Koon\n",
    "\n",
    "This document serves as a brief overview of my stage gate one project for CSCI 494-007, National Security Data Science in Action. To demonstrate my competency using Python and related data analysis libraries, I implemented an off-the-shelf neural network using TensorFlow and Keras. Given my inexperience with these two high-level libraries, I followed the \"Basic Text Classification\" developer guide from the official TensorFlow documentation. This project involved building, training, and evaluating a sequential model for binary sentiment analysis of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, os, re, shutil, string, tensorflow as tf\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Preparing the Dataset\n",
    "This project uses the Large Movie Review Dataset which was published by Andrew Mass et. al. at the Stanford AI Research Laboratory. This dataset contains 50,000 movie reviews sourced from the Internet Movie Database (IMDb), with each review containing a plain-text representation of the review and a binary label indicating whether the sentiment of the review is positive or negative. These data are already split into a balanced pair of training and testing data with 25,000 samples each. Additionally, these data are already clean and require little preprocessing save for vectorization of the text reviews and labels.\n",
    "\n",
    "The following code chunk is responsible for downloading and extracting the 'Large Movie Review Dataset' from the Stanford Artificial Intelligence laboratory. This is achieved using the keras 'get_file' method, which downloads a resource from a given URL (or from a local cache) and additionally handles extracting the underlying contents of the compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# The source URL for the Larger Movie Review Dataset\n",
    "datasetUrl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Download and extract the dataset\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", datasetUrl, untar=True, cache_dir=\".\", cache_subdir=\"\")\n",
    "\n",
    "# Get a reference to the dataset directory\n",
    "datasetDir = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['labeledBow.feat',\n 'neg',\n 'pos',\n 'unsup',\n 'unsupBow.feat',\n 'urls_neg.txt',\n 'urls_pos.txt',\n 'urls_unsup.txt']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a reference to the training data directory\n",
    "trainDir = os.path.join(datasetDir, \"train\")\n",
    "\n",
    "os.listdir(trainDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of three things: the underlying reviews (split between the `neg`, `pos`, and `unsup` directories), URL for those reviews (split between the `urls_neg.txt`, `urls_pos.txt`, and `urls_unsup.txt`), and what appears to be some sort of encoded labels for the reviews (split between `labeledBow.feat` and `unsupBow.feat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "# Get a reference to the file containing the 1181 review\n",
    "sampleFile = os.path.join(trainDir, 'pos/1181_9.txt')\n",
    "\n",
    "# Read in and print the review\n",
    "with open(sampleFile) as posReview1181:\n",
    "    print(posReview1181.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "The following section handles reading in and formatting the data into a structure suitable for preprocessing and consumption.\n",
    "Because the Dataset is structured using directories (i.e., the individual instances are represented in classes that reside in directories corresponding to their class level), it can easily load the dataset using the `text_dataset_from_directory` method.\n",
    "\n",
    "According to TensorFlow documentation, it is best practice to split data into three subsets: training, validation, and test sets. Training is used to, of course, train the model. The validation set is used to evaluate the model and its parameters, and the testing set is used for final model performance evaluation. Ideally, these three subsets are disjoint from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeDir = os.path.join(trainDir, \"unsup\")\n",
    "\n",
    "# Remove the 'unsup' directory support the format required by the `text_dataset_from_directory`\n",
    "shutil.rmtree(removeDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "batchSize = 32\n",
    "\n",
    "# Use a seed for reproducibility (42 is the seed used in TensorFlow guide)\n",
    "seed = 42\n",
    "\n",
    "# Create a new Dataset object that stores the training data\n",
    "rawTrainingDataset = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batchSize,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label 0\n",
      "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label 0\n",
      "Review b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
      "Label 1\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "for text_batch, label_batch in rawTrainingDataset.take(1):\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains two attributes per data instance; the underlying review and its classification (positive or negative). These labels are encoded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to: neg\n",
      "Label 1 corresponds to: pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to:\", rawTrainingDataset.class_names[0])\n",
    "print(\"Label 1 corresponds to:\", rawTrainingDataset.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create a new Dataset object that stores the validation data\n",
    "rawValidationDataset = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batchSize,\n",
    "    validation_split=0.2, # The size of the validation  (20% of the original )\n",
    "    subset=\"validation\",\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a new Dataset object that stores the test data (already defined for this dataset)\n",
    "rawTestDataset = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=batchSize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(rawValidationDataset) + len(rawTrainingDataset) == len(rawTestDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training\n",
    "In this section, the training, and validation get prepared for the training step. This involves three stages: standardization, tokenization, and vectorization.\n",
    "\n",
    "Standardization, or preprocessing, involves putting the data into a standard format. In our case, this includes removing punctuation and HTML characters that otherwise add complexity to each individual sample. This could include many other actions however, such as dopping missing data or removing irrelevant columns.\n",
    "\n",
    "Tokenization simply involves tokenizing the characters in the data, often times splitting by whitespace. This structures the content into individual chunks that can be semantically analysed.\n",
    "\n",
    "Vectorization involves converting tokens into vectors which can be fed into the neural network.\n",
    "\n",
    "\n",
    "> Note: It is important that the training, validation, and testing datasets are processed identically and under the same conditions to prevent training-testing skew (divergence in performance between the training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom standardization function that converts the text to lowercase, removes HTML elements, and removes punctuation\n",
    "def standardizeData(input):\n",
    "    # Convert all text to lowercase\n",
    "    lowercase = tf.strings.lower(input)\n",
    "\n",
    "    # Remove HTML elements\n",
    "    strippedHTML = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "\n",
    "    # Return the processed string with all punctuation removed\n",
    "    return tf.strings.regex_replace(strippedHTML, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing parameters\n",
    "maxFeatures = 10000\n",
    "sequenceLength = 250\n",
    "\n",
    "# Create a layer that handles standardization (using the custom `standardizeData` function), tokenization, and vectorization\n",
    "vectorizeLayer = layers.TextVectorization(\n",
    "    standardize=standardizeData,\n",
    "    max_tokens=maxFeatures,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequenceLength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing state to an unlabeled\n",
    "trainText = rawTrainingDataset.map(lambda  x, y: x)\n",
    "vectorizeLayer.adapt(trainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.', shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized Review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  86,   17,  260,    2,  222,    1,  571,   31,  229,   11, 2418,\n",
      "           1,   51,   22,   25,  404,  251,   12,  306,  282,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def vectorizeText(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorizeLayer(text), label\n",
    "\n",
    "# Test out the preprocessing on a batch of training data\n",
    "textBatch, labelBatch = next(iter(rawTrainingDataset))\n",
    "firstReview, firstLabel = textBatch[0], labelBatch[0]\n",
    "\n",
    "print(\"Review\", firstReview)\n",
    "print(\"Label\", rawTrainingDataset.class_names[firstLabel])\n",
    "print(\"Vectorized Review\", vectorizeText(firstReview, firstLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing layer appears to work as intended. The input review is processed, and then vectorized such that each token is represented with an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Dataset for Performance\n",
    "This section introduces some methods for improving training performance, specifically in regard to I/O operations. This includes `Caching` and `Prefetching`.\n",
    "\n",
    "Caching will keep the data loaded in memory, which makes retrieval of the data during the training process more expedient and efficient. In the event that there is not enough memory to fit the entire dataset, caching can also produce a more efficient on-disk cache that will still result in improved I/O performance.\n",
    "\n",
    "Prefetching will asynchronously read in the next input vector while the previous vector is being passed through the model. This reduces the 'step time' and consequently the maximum training time.\n",
    "\n",
    "> Note: Using tf.data.AUTOTUNE enables automatic tuning of the target parameter\n",
    "\n",
    "The TensorFlow guids go into more detail on how data can be optimized for better training performance: https://www.tensorflow.org/guide/data_performance#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each dataset\n",
    "trainDataset = rawTrainingDataset.map(vectorizeText)\n",
    "validationDataset = rawValidationDataset.map(vectorizeText)\n",
    "testDataset = rawTestDataset.map(vectorizeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant to indicate that a hyperparameter should be automatically tuned\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Enable caching and prefetching for the datasets\n",
    "trainDataset = trainDataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validationDataset = validationDataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "testDataset = testDataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "Here, we define the Sequential model using Keras. For this project, the model will contain five layers:\n",
    "\n",
    "Embedding: Takes the integer-encoded reviews and looks up the vector for each word-index. This is essentially a translation layer that produces the vectors with which the model will learn.\n",
    "\n",
    "GlobalAveragePooling1D: Returns a fixed-length vector for each example which is the average of the sequence dimension. This allows the model to handle input vectors of varying lengths.\n",
    "\n",
    "Dense: A densely connected layer that outputs to a single dimension.\n",
    "\n",
    "> Note: The `Dropout` layers are used to prevent over-fitting by randomly setting input values to 0 (and scaling the other values such that the total sum of values remains unchanged): https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embeddingDim = 16\n",
    "\n",
    "# Define a seed for reproducibility\n",
    "tf.random.set_seed(12345)\n",
    "\n",
    "# Define the model and its layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(maxFeatures + 1, embeddingDim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Store initial model weights for future tests\n",
    "initial_model_weights = model.get_weights()\n",
    "\n",
    "# Report a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "This section defines the loss function and optimizer that will guide the training / optimization process. The TensorFlow implements uses binary crossentropy for the loss function (https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) and the adam optimizer (A stochastic gradient descent optimizer based on 'adaptive estimation of first-order and second-order moments': https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam). I have only really worked with optimizing for MSE, however the binary crossentropy loss metric appears to be a more optimal metric for working with, well, binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer to facilitate the training process\n",
    "model.compile(\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 14s 19ms/step - loss: 0.6646 - binary_accuracy: 0.6929 - val_loss: 0.6161 - val_binary_accuracy: 0.7714\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.5499 - binary_accuracy: 0.7990 - val_loss: 0.4995 - val_binary_accuracy: 0.8220\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4455 - binary_accuracy: 0.8433 - val_loss: 0.4209 - val_binary_accuracy: 0.8470\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3788 - binary_accuracy: 0.8647 - val_loss: 0.3740 - val_binary_accuracy: 0.8600\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3357 - binary_accuracy: 0.8787 - val_loss: 0.3451 - val_binary_accuracy: 0.8672\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3060 - binary_accuracy: 0.8902 - val_loss: 0.3262 - val_binary_accuracy: 0.8716\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2817 - binary_accuracy: 0.8971 - val_loss: 0.3128 - val_binary_accuracy: 0.8738\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2626 - binary_accuracy: 0.9043 - val_loss: 0.3037 - val_binary_accuracy: 0.8762\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2465 - binary_accuracy: 0.9093 - val_loss: 0.2968 - val_binary_accuracy: 0.8776\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2315 - binary_accuracy: 0.9165 - val_loss: 0.2921 - val_binary_accuracy: 0.8790\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2185 - binary_accuracy: 0.9212 - val_loss: 0.2887 - val_binary_accuracy: 0.8816\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2077 - binary_accuracy: 0.9247 - val_loss: 0.2868 - val_binary_accuracy: 0.8814\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1972 - binary_accuracy: 0.9297 - val_loss: 0.2865 - val_binary_accuracy: 0.8812\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1877 - binary_accuracy: 0.9338 - val_loss: 0.2866 - val_binary_accuracy: 0.8828\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1791 - binary_accuracy: 0.9369 - val_loss: 0.2871 - val_binary_accuracy: 0.8842\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1696 - binary_accuracy: 0.9420 - val_loss: 0.2887 - val_binary_accuracy: 0.8840\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1625 - binary_accuracy: 0.9450 - val_loss: 0.2906 - val_binary_accuracy: 0.8842\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1548 - binary_accuracy: 0.9481 - val_loss: 0.2937 - val_binary_accuracy: 0.8828\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1467 - binary_accuracy: 0.9521 - val_loss: 0.2969 - val_binary_accuracy: 0.8830\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1417 - binary_accuracy: 0.9530 - val_loss: 0.3002 - val_binary_accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    trainDataset,\n",
    "    validation_data = validationDataset,\n",
    "    epochs = epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3345 - binary_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# Compute the loss and accuracy for the model on the testing data\n",
    "loss, accuracy = model.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Here, we consider the evaluation metrics produced in the training and validation processes. These values are retrieved from the history object that is returned from the fit() method on the model. These evaluation metrics coincide with those metrics specified in the model definition.\n",
    "\n",
    "Assuming these data are optimal for training with the sequential model previously defined, I expect the loss to degrade over each epoch (should bottom out as the model converges to a locally optimal weights), and the accuracy should simultaneously increase. In an ideal world, this behavior would be the same for when using both the training and validation data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary that stores the evaluation metrics from training/validation\n",
    "historyDict = history.history\n",
    "historyDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "acc = historyDict[\"binary_accuracy\"]\n",
    "validationAccuracy = historyDict[\"val_binary_accuracy\"]\n",
    "\n",
    "loss = historyDict[\"loss\"]\n",
    "validationLoss = historyDict[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot the training and validation loss with respect to the number if iterations\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training Loss\")\n",
    "plt.plot(epochs, validationLoss, 'b', label = \"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy with respect to the number if iterations\n",
    "plt.plot(epochs, acc, 'bo', label = \"Training Accuracy\")\n",
    "plt.plot(epochs, validationAccuracy, 'b', label = \"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It appears as though the behavior of the loss and accuracy metrics generally match my expectations, with the loss decreasing and the accuracy increasing across epochs. That said, the metrics for the validation data appear to plateau before the metrics for the training data. This indicates that after about four or five epochs, the model begins to stop improving on new data, and starts to overfit the training data. One possible way to deal with this is to terminate the training process once improvement in accuracy/loss for the validation data diminishes. This option is explored in the 'Further Exploration' section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Further Exploration (Implementing Early Stopping)\n",
    "In order to address the problem of overfitting, I have implemented the TensorFlow.keras.callbacks.EarlyStopping callback, which will monitor a evaluation metric, and terminate the fitting process when that metric stops improving. In this case, I tested this callback when monitoring both the loss and accuracy, using a patience value of 3 for both instances. The results are presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Recompile the model\n",
    "model.reset_metrics()\n",
    "model.set_weights(initial_model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the patience for the Early Stopping callback (number of iterations\n",
    "# where no improvement can occur before the fitting process is terminated)\n",
    "early_stopping_patience = 1\n",
    "\n",
    "# Define the Early Stopping callback to monitor the loss. That is, when the loss\n",
    "# stops improving for early_stopping_patience epochs, it will terminate the fitting process\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = early_stopping_patience, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "early_stopping_epochs = 20\n",
    "\n",
    "# Refit the model with the Early Stopping callback\n",
    "early_stopping_history = model.fit(\n",
    "    trainDataset,\n",
    "    validation_data = validationDataset,\n",
    "    callbacks = [early_stopping],\n",
    "    epochs = early_stopping_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary that stores the evaluation metrics from training/validation\n",
    "early_stop_history_dict = early_stopping_history.history\n",
    "early_stop_history_dict.keys()\n",
    "\n",
    "# Retrieve the metrics from the history dict\n",
    "early_stop_acc = early_stop_history_dict[\"binary_accuracy\"]\n",
    "early_stop_validation_accuracy = early_stop_history_dict[\"val_binary_accuracy\"]\n",
    "\n",
    "early_stop_loss = early_stop_history_dict[\"loss\"]\n",
    "early_stop_validation_loss = early_stop_history_dict[\"val_loss\"]\n",
    "\n",
    "# Get the number epochs that the early-stopping model trained for\n",
    "early_stopping_epochs = range(1, len(early_stopping_history.history[\"loss\"]) + 1)\n",
    "\n",
    "# Plot the training and validation loss with respect to the number if iterations\n",
    "plt.plot(early_stopping_epochs, early_stop_loss, 'bo', label = \"Training Loss\")\n",
    "plt.plot(early_stopping_epochs, early_stop_validation_loss, 'b', label = \"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy with respect to the number if iterations\n",
    "plt.plot(early_stopping_epochs, early_stop_acc, 'bo', label = \"Training Accuracy\")\n",
    "plt.plot(early_stopping_epochs, early_stop_validation_accuracy, 'b', label = \"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the loss and accuracy values for this new\n",
    "loss, accuracy = model.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As evident in the graphs above, implementing the early stopping callback resulted in a similar accuracy and loss at the cost of fewer epochs. There is clearly less deviation from the training metrics, indicating that less overfitting. This exemplifies the benefits of optimizing the training process, and begs to question how other TensorFlow/Keras callback methods might be used to more efficiently train models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
